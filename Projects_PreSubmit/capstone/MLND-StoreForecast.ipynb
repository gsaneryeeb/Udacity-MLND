{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales_sum data read successfully!\n",
      "Total number of sales_sums: 24000\n",
      "Number of features: 1\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read sales_sum data\n",
    "sales_sum_data = pd.read_csv('t_sales_sum.csv',header=0,index_col=0)\n",
    "sales_sum_data = sales_sum_data.drop_duplicates()\n",
    "n_sales_sum_features = len(sales_sum_data.columns) - 1\n",
    "n_sales_sum = len(sales_sum_data.index)\n",
    "\n",
    "print \"sales_sum data read successfully!\"\n",
    "print \"Total number of sales_sums: {}\".format(n_sales_sum)\n",
    "print \"Number of features: {}\".format(n_sales_sum_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "version = sklearn.__version__\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read training data form csv \n",
    "ads_train_data = pd.read_csv('ads_train_data.csv')\n",
    "comment_train_data = pd.read_csv('comment_train_data.csv')\n",
    "order_train_data = pd.read_csv('order_train_data.csv')\n",
    "product_train_data = pd.read_csv('product_train_data.csv')\n",
    "sales_sum_train_data = pd.read_csv('sales_sum_train_data.csv')\n",
    "sales_sum_train_data = sales_sum_train_data.drop_duplicates() # drop duplicate data\n",
    "\n",
    "# Read predict data\n",
    "ads_target_data = pd.read_csv('ads_target_data.csv')\n",
    "comment_target_data = pd.read_csv('comment_target_data.csv')\n",
    "order_target_data = pd.read_csv('order_target_data.csv')\n",
    "product_target_data = pd.read_csv('product_target_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the training data\n",
    "train_data = pd.merge(sales_sum_train_data,product_train_data,how ='left', on=['shop_id','dt'])\n",
    "train_data = pd.merge(train_data,comment_train_data,how ='left', on=['shop_id','dt'])\n",
    "train_data = pd.merge(train_data,order_train_data,how ='left', on=['shop_id','dt'])\n",
    "train_data = pd.merge(train_data,ads_train_data,how ='left', on=['shop_id','dt'])\n",
    "\n",
    "# calculate the mean\n",
    "product_on_mean = format(train_data['product_on'].mean(),'.2f')\n",
    "sum_dis_num_mean  = format(train_data['sum_dis_num'].mean(),'.2f')\n",
    "bad_percent_mean  = format(train_data['bad_percent'].mean(),'.2f')\n",
    "good_percent_mean = format(train_data['good_percent'].mean(),'.2f')\n",
    "sum_sale_amt_mean   = format(train_data['sum_sale_amt'].mean(),'.2f')\n",
    "sum_rtn_amt_mean   = format(train_data['sum_rtn_amt'].mean(),'.2f')\n",
    "sum_offer_amt_mean  = format(train_data['sum_offer_amt'].mean(),'.2f') \n",
    "sum_ord_cnt_mean  = format(train_data['sum_ord_cnt'].mean(),'.2f')\n",
    "sum_user_cnt_mean   = format(train_data['sum_user_cnt'].mean(),'.2f') \n",
    "sum_charge_mean   = format(train_data['sum_charge'].mean(),'.2f')\n",
    "sum_consume_mean = format(train_data['sum_consume'].mean(),'.2f')\n",
    "# calculate the median\n",
    "sum_charge_median   = format(train_data['sum_charge'].median(),'.2f')\n",
    "sum_consume_median = format(train_data['sum_consume'].median(),'.2f')\n",
    "\n",
    "# Handle missing values\n",
    "# Most use the mean, a small part of the median\n",
    "train_data = train_data.fillna({'product_on':product_on_mean,\n",
    "                                'sum_dis_num':sum_dis_num_mean,\n",
    "                                'bad_percent':bad_percent_mean,\n",
    "                                'good_percent':good_percent_mean,\n",
    "                                'sum_sale_amt':sum_sale_amt_mean,\n",
    "                                'sum_rtn_amt':sum_rtn_amt_mean,\n",
    "                                'sum_offer_amt':sum_offer_amt_mean,\n",
    "                                'sum_ord_cnt':sum_ord_cnt_mean,\n",
    "                                'sum_user_cnt':sum_user_cnt_mean,\n",
    "                                'sum_charge':sum_charge_median,\n",
    "                                'sum_consume':sum_consume_median\n",
    "                               }) \n",
    "\n",
    "#print(test_data[test_data.isnull().values==True]) # check NaN\n",
    "#print(train_data)\n",
    "\n",
    "# Output training data\n",
    "columns = ['shop_id','dt','sale_amt_3m','product_on','sum_dis_num','bad_percent','good_percent',\n",
    "           'sum_sale_amt','sum_rtn_amt','sum_ord_cnt','sum_user_cnt','sum_offer_amt','sum_charge','sum_consume']\n",
    "\n",
    "train_data.to_csv('train_data.csv',index = False, header = True, columns = columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge target forecast data \n",
    "\n",
    "target_data = pd.merge(order_target_data,ads_target_data, how ='left', on=['shop_id','dt'])\n",
    "target_data = pd.merge(target_data,comment_target_data, how ='left', on=['shop_id','dt'])\n",
    "target_data = pd.merge(target_data,product_target_data, how ='left', on=['shop_id','dt'])\n",
    "\n",
    "# calculate the mean and median\n",
    "target_product_on_mean = format(target_data['product_on'].mean(),'.2f')\n",
    "target_sum_dis_num_mean  = format(target_data['sum_dis_num'].mean(),'.2f')\n",
    "target_bad_percent_mean  = format(target_data['bad_percent'].mean(),'.2f')\n",
    "target_good_percent_mean = format(target_data['good_percent'].mean(),'.2f')\n",
    "target_sum_charge_median   = format(target_data['sum_charge'].median(),'.2f')\n",
    "target_sum_consume_median = format(target_data['sum_consume'].median(),'.2f')\n",
    "\n",
    "# Handle missing values\n",
    "# Most use the mean, a small part of the median\n",
    "target_data = target_data.fillna({  'product_on':target_product_on_mean,# use mean\n",
    "                                'sum_dis_num':target_sum_dis_num_mean,\n",
    "                                'bad_percent':target_bad_percent_mean,\n",
    "                                'good_percent':target_good_percent_mean,\n",
    "                                'sum_charge':target_sum_charge_median,# use median\n",
    "                                'sum_consume':target_sum_consume_median\n",
    "                               }) \n",
    "\n",
    "# print(target_data[target_data.isnull().values==True]) # check NaN data \n",
    "# print(target_data)\n",
    "\n",
    "# Output data use to predict \n",
    "columns = ['shop_id','dt','product_on','sum_dis_num','bad_percent','good_percent',\n",
    "           'sum_sale_amt','sum_rtn_amt','sum_ord_cnt','sum_user_cnt','sum_offer_amt','sum_charge','sum_consume']\n",
    "target_data.to_csv('target_data.csv',index = False, header = True, columns = columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "groups = sales_sum_data.groupby(sales_sum_data['shop_id'])\n",
    "# n_groups = len(groups)\n",
    "for name,group in groups:\n",
    "    #print(group.sort_index())\n",
    "    group.sort_index().plot(y='sale_amt_3m')\n",
    "#groups.plot(y='sale_amt_3m')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beanchmark\n",
    "# Mean of sale_amt_3m for each store for 8 months\n",
    "grouped = sales_sum_data['sale_amt_3m'].groupby(sales_sum_data['shop_id'])\n",
    "print(grouped.mean()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "train = pd.read_csv('train_data.csv')\n",
    "target = pd.read_csv('target_data.csv')\n",
    "#selected_features=['consume','good','bad','sale','product','dis','cnmt','ord_cnt','user_cnt','offer_amt']\n",
    "selected_features=['product_on','sum_dis_num','bad_percent','good_percent',\n",
    "                   'sum_sale_amt','sum_rtn_amt','sum_ord_cnt','sum_user_cnt',\n",
    "                   'sum_offer_amt','sum_charge','sum_consume']\n",
    "features = train[selected_features]\n",
    "X_target = target[selected_features]\n",
    "\n",
    "sale_amt_3m = train['sale_amt_3m']\n",
    "\n",
    "print \"The dataset has {} data points with {} variables each.\".format(*features.shape)\n",
    "print \"The predict dataset has {} data points with {} variables each.\".format(*X_target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From sklearn.cross_validation import data splitter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = features\n",
    "y = sale_amt_3m\n",
    "\n",
    "# A random sample of 25% of the data to build test samples, the rest as a training sample\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=33)\n",
    "\n",
    "# Success\n",
    "print \"Training and testing split was successful.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Minimum sale_amt_3m of the data\n",
    "minimum_sale_amt_3m = np.min(sale_amt_3m)\n",
    "\n",
    "# TODO: Maximum sale_amt_3m of the data\n",
    "maximum_sale_amt_3m = np.max(sale_amt_3m)\n",
    "\n",
    "# TODO: Mean sale_amt_3m of the data\n",
    "mean_sale_amt_3m = np.mean(sale_amt_3m)\n",
    "\n",
    "# TODO: Median sale_amt_3m of the data\n",
    "median_sale_amt_3m = np.median(sale_amt_3m)\n",
    "\n",
    "# TODO: Standard deviation of sale_amt_3m of the data\n",
    "std_sale_amt_3m = np.std(sale_amt_3m)\n",
    "\n",
    "# Show the calculated statistics\n",
    "print \"Statistics for StoreForecast dataset:\\n\"\n",
    "print \"Minimum sale_amt_3m: ${:,.2f}\".format(minimum_sale_amt_3m)\n",
    "print \"Maximum sale_amt_3m: ${:,.2f}\".format(maximum_sale_amt_3m)\n",
    "print \"Mean sale_amt_3m: ${:,.2f}\".format(mean_sale_amt_3m)\n",
    "print \"Median sale_amt_3m ${:,.2f}\".format(median_sale_amt_3m)\n",
    "print \"Standard deviation of sale_amt_3m: ${:,.2f}\".format(std_sale_amt_3m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "ss_X = StandardScaler()\n",
    "ss_y = StandardScaler()\n",
    "\n",
    "# Standard Scaler\n",
    "X_train = ss_X.fit_transform(X_train)\n",
    "\n",
    "X_test = ss_X.fit_transform(X_test)\n",
    "y_train = ss_y.fit_transform(y_train.reshape(-1,1))\n",
    "y_test = ss_y.fit_transform(y_test.reshape(-1,1))\n",
    "X_target = ss_X.fit_transform(X_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_y_predict = lr.predict(X_test)\n",
    "\n",
    "#lr.fit(X_train_ss,y_train_ss)\n",
    "#lr_y_predict_ss = lr.predict(X_test_ss)\n",
    "# print(lr_y_predict)\n",
    "# print(ss_y.inverse_transform(lr_y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,r2_score,mean_squared_error\n",
    "\n",
    "print 'The value of R-squared of LinearRegression is',r2_score(y_test,lr_y_predict)\n",
    "#print 'The mean squared error of LinearRegression is',mean_squared_error(y_test,lr_y_predict)\n",
    "#print 'The mean absoluate error of LinearRegression is',mean_absolute_error(y_test,lr_y_predict)\n",
    "#print 'The value of R-squared of LinearRegression is',r2_score(y_test_ss,lr_y_predict_ss)\n",
    "print 'The mean squared error of LinearRegression is',mean_squared_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(lr_y_predict))\n",
    "print 'The mean absoluate error of LinearRegression is',mean_absolute_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(lr_y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# use linear kernel \n",
    "linear_svr = SVR(kernel='linear')\n",
    "linear_svr.fit(X_train, y_train)\n",
    "linear_svr_y_predict = linear_svr.predict(X_test)\n",
    "\n",
    "# use ploy kernel\n",
    "#poly_svr = SVR(kernel='ploy')\n",
    "#poly_svr.fit(X_train, y_train)\n",
    "#ploy_svr_y_predit = poly_svr.predict(X_test)\n",
    "\n",
    "# use rbf kernel\n",
    "rbf_svr = SVR(kernel='rbf')\n",
    "rbf_svr.fit(X_train, y_train)\n",
    "rbf_svr_y_predit = rbf_svr.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'The value of R-squared of Linear SVR is',linear_svr.score(X_test,y_test)\n",
    "print 'The mean squared error of Linear SVR is',mean_squared_error(ss_y.inverse_transform(y_test),\n",
    "                                                                         ss_y.inverse_transform(linear_svr_y_predict))\n",
    "print 'The mean absoluate error of Linear SVR is',mean_absolute_error(ss_y.inverse_transform(y_test),\n",
    "                                                                          ss_y.inverse_transform(linear_svr_y_predict))\n",
    "#print 'The value of R-squared of Linear SVR is',poly_svr.score(X_test,y_test)\n",
    "#print 'The mean squared error of Linear SVR is',mean_squared_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(ploy_svr_y_predit))\n",
    "#print 'The mean absoluate error of Linear SVR is',mean_absolute_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(ploy_svr_y_predit))\n",
    "print 'The value of R-squared of RBF SVR is',rbf_svr.score(X_test,y_test)\n",
    "print 'The mean squared error of RBF SVR is',mean_squared_error(ss_y.inverse_transform(y_test),\n",
    "                                                                         ss_y.inverse_transform(rbf_svr_y_predit))\n",
    "print 'The mean absoluate error of RBF SVR is',mean_absolute_error(ss_y.inverse_transform(y_test),\n",
    "                                                                          ss_y.inverse_transform(rbf_svr_y_predit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# init KNeighborsRegressor,use weights='uniform'\n",
    "uni_knr = KNeighborsRegressor(weights='uniform')\n",
    "uni_knr.fit(X_train,y_train)\n",
    "uni_knr_y_predict =uni_knr.predict(X_test)\n",
    "\n",
    "# init KNeighborsRegressor,weight points by the inverse of their distance weights='distance'\n",
    "dis_knr = KNeighborsRegressor(weights='distance')\n",
    "dis_knr.fit(X_train,y_train)\n",
    "dis_knr_y_predict =dis_knr.predict(X_test)\n",
    "\n",
    "print 'The value of R-squared of uniform-weights KNeighborsRegressor is',uni_knr.score(X_test,y_test)\n",
    "print 'The mean squared error of uniform-weights KNeighborsRegressor is',mean_squared_error(ss_y.inverse_transform(y_test),\n",
    "                                                                          ss_y.inverse_transform(uni_knr_y_predict))\n",
    "print 'The mean absoluate error of uniform-weights KNeighborsRegressor is',mean_absolute_error(ss_y.inverse_transform(y_test),\n",
    "                                                                          ss_y.inverse_transform(uni_knr_y_predict))\n",
    "\n",
    "print 'The value of R-squared of distance-weights KNeighborsRegressor is',dis_knr.score(X_test,y_test)\n",
    "print 'The mean squared error of distance-weights KNeighborsRegressor is',mean_squared_error(ss_y.inverse_transform(y_test),\n",
    "                                                                          ss_y.inverse_transform(dis_knr_y_predict))\n",
    "print 'The mean absoluate error of distance-weights KNeighborsRegressor is',mean_absolute_error(ss_y.inverse_transform(y_test),\n",
    "                                                                          ss_y.inverse_transform(dis_knr_y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(X_train, y_train)\n",
    "dtr_y_predict = dtr.predict(X_test)\n",
    "\n",
    "print 'The value of R-squared of DecisionTreeRegressor is',dtr.score(X_test,y_test)\n",
    "print 'The mean squared error of DecisionTreeRegressor is',mean_squared_error(ss_y.inverse_transform(y_test),\n",
    "                                                                          ss_y.inverse_transform(dtr_y_predict))\n",
    "print 'The mean absoluate error of DecisionTreeRegressor is',mean_absolute_error(ss_y.inverse_transform(y_test),\n",
    "                                                                          ss_y.inverse_transform(dtr_y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use enemble\n",
    "from sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor,GradientBoostingRegressor\n",
    "\n",
    "# use RandomForestRegressor\n",
    "rtf = RandomForestRegressor()\n",
    "rtf.fit(X_train,y_train)\n",
    "rtf_y_predict = rtf.predict(X_test)\n",
    "\n",
    "# use ExtraTreesRegressor\n",
    "etr = ExtraTreesRegressor()\n",
    "etr.fit(X_train,y_train)\n",
    "etr_y_predict = etr.predict(X_test)\n",
    "\n",
    "# use GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor()\n",
    "gbr.fit(X_train,y_train)\n",
    "gbr_y_predict = gbr.predict(X_test)\n",
    "\n",
    "print 'The value of R-squared of RandomForestRegressor is',rtf.score(X_test,y_test)\n",
    "print 'The mean squared error of RandomForestRegressor is',mean_squared_error(ss_y.inverse_transform(y_test),\n",
    "                                                                          ss_y.inverse_transform(rtf_y_predict))\n",
    "print 'The mean absoluate error of RandomForestRegressor is',mean_absolute_error(ss_y.inverse_transform(y_test),\n",
    "                                                                          ss_y.inverse_transform(rtf_y_predict))\n",
    "\n",
    "print 'The value of R-squared of ExtraTreesRegressor is',etr.score(X_test,y_test)\n",
    "print 'The mean squared error of ExtraTreesRegressor is',mean_squared_error(ss_y.inverse_transform(y_test),\n",
    "                                                                          ss_y.inverse_transform(etr_y_predict))\n",
    "print 'The mean absoluate error of ExtraTreesRegressor is',mean_absolute_error(ss_y.inverse_transform(y_test),\n",
    "                                                                          ss_y.inverse_transform(etr_y_predict))\n",
    "\n",
    "print 'The value of R-squared of GradientBoostingRegressor is',gbr.score(X_test,y_test)\n",
    "print 'The mean squared error of GradientBoostingRegressor is',mean_squared_error(ss_y.inverse_transform(y_test),\n",
    "                                                                          ss_y.inverse_transform(gbr_y_predict))\n",
    "print 'The mean absoluate error of GradientBoostingRegressor is',mean_absolute_error(ss_y.inverse_transform(y_test),\n",
    "                                                                          ss_y.inverse_transform(gbr_y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The MAE score of the SVR with RBF kernel is the highest and this algorithm is chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to csv file\n",
    "y_test_predict = rbf_svr.predict(X_target)\n",
    "y_test_predict = ss_y.inverse_transform(y_test_predict)\n",
    "jd_best_submission =pd.DataFrame({'shop_id':target['shop_id'],'sale_amt_3m':y_test_predict,})\n",
    "\n",
    "# set columns \n",
    "columns=['shop_id','sale_amt_3m']\n",
    "\n",
    "# output csv， index，header，columns\n",
    "jd_best_submission.to_csv('Sales_Forecast_Upload_3.csv',index=False, header = False, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Import 'mean_absolute_error'\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "def performance_metric(y_true, y_predict):\n",
    "    \"\"\" Calculates and returns the performance score between \n",
    "        true and predicted values based on the metric chosen. \"\"\"\n",
    "    \n",
    "    # TODO: Calculate the performance score between 'y_true' and 'y_predict'\n",
    "    score = mean_absolute_error(y_true, y_predict)\n",
    "    \n",
    "    # Return the score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def svr_param_selection(X, y):\n",
    "    # Create cross-validation sets from the training data\n",
    "    # sklearn version 0.18: ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, random_state=None)\n",
    "    # sklearn versiin 0.17: ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\n",
    "    cv_sets = ShuffleSplit(n_splits = 10, test_size = 0.20, random_state = 0)\n",
    "    # cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\n",
    "    \n",
    "    # TODO: Create a SVR  object\n",
    "    regressor = SVR(kernel='rbf')\n",
    "    \n",
    "    # Create a dictionary for the parameter Cs,gammas\n",
    "    Cs = [0.001,0.01,0.1,1,10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma': gammas}\n",
    "    \n",
    "    #Transform 'performance_metric' into a scoring function using 'make_scorer' \n",
    "    scoring_fnc = make_scorer(performance_metric)\n",
    "    \n",
    "    grid_search = GridSearchCV(regressor, param_grid, scoring = scoring_fnc, cv=cv_sets)\n",
    "    \n",
    "    # Fit the grid search object to the data to compute the optimal model\n",
    "    grid_search.fit(X,y)\n",
    "    \n",
    "    # Return the optimal model after fitting the data\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the training data to the model using grid search\n",
    "svr_opt = svr_param_selection(X_train, y_train)\n",
    "print(\"Parameter 'C' is {} for the optimal model\".format(svr_opt.get_params()['C']))\n",
    "print(\"Parameter 'gamma' is {} for the optimal model\".format(svr_opt.get_params()['gamma']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to csv file\n",
    "y_predict_opt = svr_opt.predict(X_target)\n",
    "y_predict_opt = ss_y.inverse_transform(y_predict_opt)\n",
    "jd_best_submission_opt =pd.DataFrame({'shop_id':target['shop_id'],'sale_amt_3m':y_predict_opt,})\n",
    "\n",
    "# set columns \n",
    "columns=['shop_id','sale_amt_3m']\n",
    "\n",
    "# output csv， index，header，columns\n",
    "jd_best_submission_opt.to_csv('Sales_Forecast_Upload_3_opt.csv',index=False, header = False, columns=columns)\n",
    "print \"Create csv file successful.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales_sum data read successfully!\n",
      "Total number of sales_sums: 1\n",
      "Number of features: 27000\n"
     ]
    }
   ],
   "source": [
    "vs_data = pd.read_csv('vs.csv',header=0,index_col=0)\n",
    "vs_data = vs_data.drop_duplicates()\n",
    "n_vs_features = len(vs_data.columns) - 1\n",
    "n_vs = len(vs_data.index)\n",
    "\n",
    "print \"sales_sum data read successfully!\"\n",
    "print \"Total number of sales_sums: {}\".format(n_vs_features)\n",
    "print \"Number of features: {}\".format(n_vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saneryee/anaconda2/lib/python2.7/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0f42ef5ec17a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sale_amt_3m'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#groups.plot(y='sale_amt_3m')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "vss = vs_data.groupby(vs_data['shop_id'])\n",
    "# n_groups = len(groups)\n",
    "for name,vs in vss:\n",
    "    #print(group.sort_index())\n",
    "    vs.sort_index().plot(y='sale_amt_3m')\n",
    "#groups.plot(y='sale_amt_3m')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
